#!/bin/bash

# Validation script for Advanced Schema Evolution recipe
# This script validates that the schema evolution operations work correctly

set -e

echo "ğŸ” Validating Advanced Schema Evolution Recipe"
echo "=============================================="

# Check if Python is available
if ! command -v python &> /dev/null; then
    echo "âŒ Python is not installed or not in PATH"
    exit 1
fi

# Check if required packages can be imported
echo "ğŸ“¦ Checking dependencies..."
python -c "
try:
    import pyspark
    from pyspark.sql import SparkSession
    print('âœ… PySpark available')
except ImportError as e:
    print(f'âŒ PySpark not available: {e}')
    exit(1)
"

# Create temporary directory for testing
TEST_DIR="/tmp/schema-evolution-test"
rm -rf "$TEST_DIR"
mkdir -p "$TEST_DIR"

echo "ğŸ§ª Running basic functionality test..."

# Run a simplified version of the evolution demo
python -c "
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from datetime import datetime
import os

# Initialize Spark
spark = SparkSession.builder.appName('SchemaEvolutionTest').getOrCreate()
spark.sparkContext.setLogLevel('ERROR')

# Create test data
schema = StructType([
    StructField('id', StringType(), False),
    StructField('name', StringType(), True),
    StructField('value', IntegerType(), True)
])

data = [('test1', 'Alice', 100), ('test2', 'Bob', 200)]
df = spark.createDataFrame(data, schema)

# Test basic Delta write
test_path = '/tmp/schema-evolution-test/basic-table'
df.write.format('delta').mode('overwrite').save(test_path)

# Test read back
read_df = spark.read.format('delta').load(test_path)
count = read_df.count()

print(f'âœ… Basic Delta operations working. Records: {count}')

# Test schema evolution (add column)
spark.sql(f'''
ALTER TABLE delta.\`{test_path}\`
ADD COLUMN new_field STRING DEFAULT \"test\"
''')

# Verify new column exists
evolved_df = spark.read.format('delta').load(test_path)
columns = evolved_df.columns

if 'new_field' in columns:
    print('âœ… Schema evolution (add column) working')
else:
    print('âŒ Schema evolution failed')
    exit(1)

spark.stop()
print('âœ… All basic tests passed!')
"

echo "ğŸ¯ Running advanced validation..."

# Test schema evolution manager import
python -c "
import sys
import os
sys.path.insert(0, os.path.dirname(__file__))

try:
    # Import the main module
    exec(open('solution.py').read())
    print('âœ… Solution script syntax is valid')
except SyntaxError as e:
    print(f'âŒ Syntax error in solution.py: {e}')
    exit(1)
except ImportError as e:
    print(f'âš ï¸  Import warning (expected in test env): {e}')
except Exception as e:
    print(f'âŒ Other error: {e}')
    exit(1)
"

echo ""
echo "ğŸ“Š Validation Results:"
echo "======================"
echo "âœ… Python environment check passed"
echo "âœ… PySpark availability confirmed"
echo "âœ… Basic Delta operations working"
echo "âœ… Schema evolution operations functional"
echo "âœ… Script syntax validation passed"
echo ""
echo "ğŸ‰ All validations passed! The recipe is ready to use."
echo ""
echo "ğŸ’¡ To run the full demo:"
echo "   python solution.py"