#!/bin/bash

# Validation script for Streaming CDC Pipeline recipe
# This script validates that the CDC streaming operations work correctly

set -e

echo "üîç Validating Streaming CDC Pipeline Recipe"
echo "==========================================="

# Check if Python is available
if ! command -v python &> /dev/null; then
    echo "‚ùå Python is not installed or not in PATH"
    exit 1
fi

# Check if required packages can be imported
echo "üì¶ Checking dependencies..."
python -c "
try:
    import pyspark
    from pyspark.sql import SparkSession
    print('‚úÖ PySpark available')
except ImportError as e:
    print(f'‚ùå PySpark not available: {e}')
    exit(1)

try:
    import json
    print('‚úÖ JSON support available')
except ImportError as e:
    print(f'‚ùå JSON support missing: {e}')
    exit(1)
"

# Create temporary directory for testing
TEST_DIR="/tmp/cdc-validation"
rm -rf "$TEST_DIR"
mkdir -p "$TEST_DIR"

echo "üß™ Running basic functionality test..."

# Test basic streaming and CDC components
python -c "
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *
from datetime import datetime
import json
import os

# Initialize Spark
spark = SparkSession.builder.appName('CDCValidationTest').getOrCreate()
spark.sparkContext.setLogLevel('ERROR')

print('‚úÖ Spark session created')

# Test CDC event schema
cdc_schema = StructType([
    StructField('table_name', StringType(), True),
    StructField('operation', StringType(), True),
    StructField('before', StringType(), True),
    StructField('after', StringType(), True),
    StructField('timestamp', TimestampType(), True),
    StructField('transaction_id', StringType(), True),
    StructField('primary_key', StringType(), True)
])

print('‚úÖ CDC schema defined')

# Test sample data creation
sample_data = {
    'table_name': 'customers',
    'operation': 'INSERT',
    'before': None,
    'after': json.dumps({'customer_id': 'CUST_001', 'name': 'John Doe'}),
    'timestamp': datetime.now(),
    'transaction_id': 'TXN_1001',
    'primary_key': json.dumps({'customer_id': 'CUST_001'})
}

sample_df = spark.createDataFrame([sample_data], cdc_schema)
count = sample_df.count()
print(f'‚úÖ Sample CDC data created: {count} records')

# Test JSON parsing
parsed_df = sample_df.withColumn('after_parsed', from_json(col('after'), StructType([
    StructField('customer_id', StringType(), True),
    StructField('name', StringType(), True)
])))

result = parsed_df.select('after_parsed.customer_id', 'after_parsed.name').collect()[0]
print(f'‚úÖ JSON parsing works: {result[\"customer_id\"]}, {result[\"name\"]}')

# Test Delta table creation
test_table_path = '/tmp/cdc-validation/test-table'
sample_df.write.format('delta').mode('overwrite').save(test_table_path)

read_df = spark.read.format('delta').load(test_table_path)
read_count = read_df.count()
print(f'‚úÖ Delta table operations work: {read_count} records')

spark.stop()
print('‚úÖ All basic tests passed!')
"

echo "üéØ Running streaming validation..."

# Test streaming components (basic)
python -c "
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *
import json
import os
import tempfile

# Initialize Spark
spark = SparkSession.builder.appName('StreamingValidationTest').getOrCreate()
spark.sparkContext.setLogLevel('ERROR')

print('üß™ Testing streaming components...')

# Create test data directory
test_data_dir = '/tmp/cdc-stream-test'
os.makedirs(test_data_dir, exist_ok=True)

# Generate test CDC events
test_events = []
for i in range(5):
    event = {
        'table_name': 'customers',
        'operation': 'INSERT',
        'before': None,
        'after': json.dumps({
            'customer_id': f'CUST_{i:03d}',
            'name': f'Customer {i}',
            'email': f'customer{i}@example.com'
        }),
        'timestamp': '2024-01-01T10:00:00Z',
        'transaction_id': f'TXN_{1000+i}',
        'primary_key': json.dumps({'customer_id': f'CUST_{i:03d}'})
    }
    test_events.append(event)

    # Write to file
    with open(f'{test_data_dir}/event_{i}.json', 'w') as f:
        json.dump(event, f)

print(f'‚úÖ Generated {len(test_events)} test events')

# Test file-based streaming read
cdc_schema = StructType([
    StructField('table_name', StringType(), True),
    StructField('operation', StringType(), True),
    StructField('before', StringType(), True),
    StructField('after', StringType(), True),
    StructField('timestamp', TimestampType(), True),
    StructField('transaction_id', StringType(), True),
    StructField('primary_key', StringType(), True)
])

try:
    stream_df = (spark.readStream
                .format('json')
                .schema(cdc_schema)
                .load(test_data_dir))

    print('‚úÖ Streaming DataFrame created')

    # Test basic transformations
    transformed_df = stream_df.withColumn('processed_at', current_timestamp())
    transformed_df = transformed_df.withColumn('after_parsed',
        from_json(col('after'), StructType([
            StructField('customer_id', StringType(), True),
            StructField('name', StringType(), True),
            StructField('email', StringType(), True)
        ])))

    print('‚úÖ Streaming transformations applied')

except Exception as e:
    print(f'‚ùå Streaming test failed: {e}')
    spark.stop()
    exit(1)

spark.stop()
print('‚úÖ Streaming validation passed!')
"

echo "üîç Running script syntax validation..."

# Test script imports and basic syntax
python -c "
import sys
import os
import ast

# Read and parse the solution script
try:
    with open('solution.py', 'r') as f:
        script_content = f.read()

    # Parse the AST
    tree = ast.parse(script_content)
    print('‚úÖ Solution script syntax is valid')

    # Check for required classes
    classes_found = []
    for node in ast.walk(tree):
        if isinstance(node, ast.ClassDef):
            classes_found.append(node.name)

    required_classes = ['CDCEvent', 'StreamingCDCProcessor']
    for cls in required_classes:
        if cls in classes_found:
            print(f'‚úÖ Required class {cls} found')
        else:
            print(f'‚ùå Required class {cls} not found')

except SyntaxError as e:
    print(f'‚ùå Syntax error in solution.py: {e}')
    exit(1)
except Exception as e:
    print(f'‚ùå Error validating solution.py: {e}')
    exit(1)
"

echo ""
echo "üìä Validation Results:"
echo "======================"
echo "‚úÖ Python environment check passed"
echo "‚úÖ PySpark and JSON support confirmed"
echo "‚úÖ CDC schema and data handling works"
echo "‚úÖ Delta table operations functional"
echo "‚úÖ Basic streaming components work"
echo "‚úÖ Script syntax validation passed"
echo "‚úÖ Required classes present"
echo ""
echo "üéâ All validations passed! The CDC pipeline recipe is ready to use."
echo ""
echo "üí° To run the full demo:"
echo "   python solution.py"
echo ""
echo "üí° For production deployment:"
echo "   - Configure proper Kafka brokers"
echo "   - Set up monitoring and alerting"
echo "   - Configure checkpoint locations"
echo "   - Adjust batch sizes and intervals"